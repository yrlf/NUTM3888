---
title: "major_project_3"
output: html_document
---
```{r}
# test branch

# test branch 2
```


```{r library}
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files
#library(janitor)   # data cleaning 
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
# missing values
#library(naniar)
#library(knitr)
#library(ggpubr) # ggplot arrangement
#ploting 
library(gridExtra)
library(kableExtra)
#outlier
#library(univOutl)
# tree methods
#library(tourr)
#library(RColorBrewer)
#library(plotly)
#library(htmltools)
library(performanceEstimation)# for SMOTE
library(rpart)
library(rpart.plot)
library(rattle) #fancyRpartPlot
library(Rtsne)
library(randomForest)
library(neuralnet)
library(e1071)# SVM regression
library(mltools)
library(data.table)
library(skimr)
library(smotefamily)
library(broom)
library(jtools)
```


```{r load data}
load("tech_data.Rdata") # load cleaned data from John's code, make sure you have the Rdata file within the working directory
```

```{r dataset fliter}
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC","PHDCMWBC","EXLWMBC","EXLWVBC",
 "SYSTOL","DIASTOL","TRIGRESB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","HDLCHREB","CVDMEDST","APOBRESB") # add/remove variables that are interested

dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 3488 obs x 13 variables

skim(dat2$DIABBC)
```

```{r create factor scores}
dat20<-dat2 %>% mutate(
Sys_score=ifelse(SYSTOL<120,0,ifelse(SYSTOL<130,1,ifelse(SYSTOL<140,2,ifelse(SYSTOL>=998,NA,3)))),
Dis_score=ifelse(DIASTOL<80,0,ifelse(DIASTOL<90,1,ifelse(DIASTOL<100,2,ifelse(DIASTOL>=998,NA,3)))),  
Tri_score=ifelse(TRIGRESB<4,0,ifelse(TRIGRESB<6,1,ifelse(TRIGRESB>=97,NA,2))),
Chol_score=ifelse(CHOLRESB<4,0,ifelse(CHOLRESB<7,1,ifelse(CHOLRESB>=97,NA,2))),
LDL_score=ifelse(LDLRESB<5,0,ifelse(LDLRESB<8,1,ifelse(LDLRESE>=97,NA,2))),
Glu_score=ifelse(GLUCFREB<4,0,ifelse(GLUCFREB<6,1,ifelse(GLUCFREB>=97,NA,2))),
HDL_score=ifelse(HDLCHREB>=7,NA,ifelse(HDLCHREB>=5,0,ifelse(HDLCHREB>=2,1,2))),

)

str(dat20)
```

```{r create obesity scores}
# dat3 contains obesity scores that are manually created for 4 diseases
# the higher score the more likely to be obesity

dat3<-dat20 %>% mutate(
  dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
  cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
  sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
  hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),2,ifelse(CVDMEDST==4,1,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat3$cvd_score) # the final score is highly unbalanced
nrow(dat3[which(dat3$CVDMEDST==5),]) # the final score of 2790 observations are 0

skim(dat3)

dat4<-dat20 %>% mutate(
  dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
  cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
  sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
  hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),2,ifelse(CVDMEDST==4,1,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat4$cho_score) # the final score is highly unbalanced
nrow(dat4[which(dat4$HCHOLBC==5),]) # the final score of 2790 observations are 0

skim(dat4)

dat5<-dat20 %>% mutate(
  dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
  cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
  sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
  hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),2,ifelse(CVDMEDST==4,1,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat5$sug_score) # the final score is highly unbalanced
nrow(dat5[which(dat5$HSUGBC==5),]) # the final score of 2790 observations are 0

skim(dat5)

dat6<-dat20 %>% mutate(
  dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
  cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
  sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
  hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
  cvd_score= ifelse((CVDMEDST==1|CVDMEDST==2|CVDMEDST==3),2,ifelse(CVDMEDST==4,1,NA)),
  final_score = dia_score+cho_score+sug_score+hyp_score+cvd_score
)

hist(dat6$hyp_score) # the final score is highly unbalanced
nrow(dat6[which(dat5$HYPBC==5),]) # the final score of 2790 observations are 0

skim(dat6)
```


```{r SMOTE function from DWmR package}
SMOTE <- function(form,data,
                  perc.over=200,k=5,
                  perc.under=200,
                  learner=NULL,...
                  )
  
  # INPUTS:
  # form a model formula
  # data the original training set (with the unbalanced distribution)
  # minCl  the minority class label
  # per.over/100 is the number of new cases (smoted cases) generated
  #              for each rare case. If perc.over < 100 a single case
  #              is generated uniquely for a randomly selected perc.over
  #              of the rare cases
  # k is the number of neighbours to consider as the pool from where
  #   the new examples are generated
  # perc.under/100 is the number of "normal" cases that are randomly
  #                selected for each smoted case
  # learner the learning system to use.
  # ...  any learning parameters to pass to learner
{

  # the column where the target variable is
  tgt <- which(names(data) == as.character(form[[2]]))
  minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
  
  # get the cases of the minority class
  minExs <- which(data[,tgt] == minCl)

  # generate synthetic cases from these minExs
  if (tgt < ncol(data)) {
      cols <- 1:ncol(data)
      cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
      data <-  data[,cols]
  }
  newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
  if (tgt < ncol(data)) {
      newExs <- newExs[,cols]
      data <- data[,cols]
  }
  
  # get the undersample of the "majority class" examples
  selMaj <- sample((1:NROW(data))[-minExs],
                   as.integer((perc.under/100)*nrow(newExs)),
                   replace=T)

  # the final data set (the undersample+the rare cases+the smoted exs)
  newdataset <- rbind(data[selMaj,],data[minExs,],newExs)

  # learn a model if required
  if (is.null(learner)) return(newdataset)
  else do.call(learner,list(form,newdataset,...))
}



# ===================================================
# Obtain a set of smoted examples for a set of rare cases.
# L. Torgo, Feb 2010
# ---------------------------------------------------
smote.exs <- function(data,tgt,N,k)
  # INPUTS:
  # data are the rare cases (the minority "class" cases)
  # tgt is the name of the target variable
  # N is the percentage of over-sampling to carry out;
  # and k is the number of nearest neighours to use for the generation
  # OUTPUTS:
  # The result of the function is a (N/100)*T set of generated
  # examples with rare values on the target
{
  nomatr <- c()
  T <- matrix(nrow=dim(data)[1],ncol=dim(data)[2]-1)
  for(col in seq.int(dim(T)[2]))
    if (class(data[,col]) %in% c('factor','character')) {
      T[,col] <- as.integer(data[,col])
      nomatr <- c(nomatr,col)
    } else T[,col] <- data[,col]
  
  if (N < 100) { # only a percentage of the T cases will be SMOTEd
    nT <- NROW(T)
    idx <- sample(1:nT,as.integer((N/100)*nT))
    T <- T[idx,]
    N <- 100
  }

  p <- dim(T)[2]
  nT <- dim(T)[1]

  ranges <- apply(T,2,max)-apply(T,2,min)
  
  nexs <-  as.integer(N/100) # this is the number of artificial exs generated
                                        # for each member of T
  new <- matrix(nrow=nexs*nT,ncol=p)    # the new cases

  for(i in 1:nT) {

    # the k NNs of case T[i,]
    xd <- scale(T,T[i,],ranges)
    for(a in nomatr) xd[,a] <- xd[,a]==0
    dd <- drop(xd^2 %*% rep(1, ncol(xd)))
    kNNs <- order(dd)[2:(k+1)]

    for(n in 1:nexs) {
      # select randomly one of the k NNs
      neig <- sample(1:k,1)

      ex <- vector(length=ncol(T))

      # the attribute values of the generated case
      difs <- T[kNNs[neig],]-T[i,]
      new[(i-1)*nexs+n,] <- T[i,]+runif(1)*difs
      for(a in nomatr)
        new[(i-1)*nexs+n,a] <- c(T[kNNs[neig],a],T[i,a])[1+round(runif(1),0)]

    }
  }
  newCases <- data.frame(new)
  for(a in nomatr)
    newCases[,a] <- factor(newCases[,a],levels=1:nlevels(data[,a]),labels=levels(data[,a]))

  newCases[,tgt] <- factor(rep(data[1,tgt],nrow(newCases)),levels=levels(data[,tgt]))
  colnames(newCases) <- colnames(data)
  newCases
}
```



```{r smote}
dat33<-dat3[,1:18] # do NOT include DIABBC and sores other than dia_score
dat33$CVDMEDST<-droplevels(dat33$CVDMEDST)
dat33<-na.omit(dat33)
table(dat33$CVDMEDST)


dat33.balanced <-smote(CVDMEDST~., dat33, perc.over=600, perc.under=2)

table(dat33.balanced$CVDMEDST)


dat33.balanced<-dat33.balanced %>% mutate(
  cvd_score= ifelse(CVDMEDST==4,0,1),
)

dat33.balanced$cvd_score<-as.factor(dat33.balanced$cvd_score)


# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat33.balanced$cvd_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33.balanced[training.samples, ]
test.data <- dat33.balanced[-training.samples, ]

dat33.balanced

#model<-nnet::multinom(dia_score~.-DIABBC, data=train.data)
model<-nnet::multinom(cvd_score~.-CVDMEDST, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$DIABBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$cvd_score)

mean(predict_class==test.data$cvd_score)





training.samples <- dat33$CVDMEDST %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat33[training.samples, ]
test.data <- dat33[-training.samples, ]

model<-nnet::multinom(CVDMEDST~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$CVDMEDST)



mean(predict_class==test.data$CVDMEDST)

# 
# summary(cvd_model)

dat44<-dat4[,1:18] # do NOT include DIABBC and sores other than dia_score
dat44$HCHOLBC<-droplevels(dat44$HCHOLBC)
dat44<-na.omit(dat44)
table(dat44$HCHOLBC)


dat44.balanced <-smote(HCHOLBC~., dat44, perc.over=600, perc.under=2)

table(dat44.balanced$HCHOLBC)


dat44.balanced<-dat44.balanced %>% mutate(
  cho_score= ifelse(HCHOLBC==5, 0, 1),
)

dat44.balanced$cho_score<-as.factor(dat44.balanced$cho_score)


# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat44.balanced$cho_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat44.balanced[training.samples, ]
test.data <- dat44.balanced[-training.samples, ]

dat44.balanced

#model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$HCHOLBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$cho_score)

mean(predict_class==test.data$cho_score)





training.samples <- dat44$HCHOLBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat44[training.samples, ]
test.data <- dat44[-training.samples, ]

model<-nnet::multinom(HCHOLBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$HCHOLBC)



mean(predict_class==test.data$HCHOLBC)

# 
# summary(cho_model)

#sug_score
dat55<-dat5[,1:18] # do NOT include DIABBC and sores other than dia_score
dat55$HSUGBC<-droplevels(dat55$HSUGBC)
dat55<-na.omit(dat55)
table(dat55$HSUGBC)


dat55.balanced <-smote(HSUGBC~., dat55, perc.over=600, perc.under=2)

table(dat55.balanced$HSUGBC)


dat55.balanced<-dat55.balanced %>% mutate(
  sug_score= ifelse(HSUGBC==3, 0, 1),
)

dat55.balanced$sug_score<-as.factor(dat55.balanced$sug_score)

set.seed(2021)
training.samples <- dat55.balanced$sug_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat55.balanced[training.samples, ]
test.data <- dat55.balanced[-training.samples, ]

dat55.balanced

#model<-nnet::multinom(sug_score~.-HSUGBC, data=train.data)
model<-nnet::multinom(sug_score~.-HSUGBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$HSUGBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$sug_score)

mean(predict_class==test.data$sug_score)





training.samples <- dat55$HSUGBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat55[training.samples, ]
test.data <- dat55[-training.samples, ]

model<-nnet::multinom(HSUGBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$HSUGBC)



mean(predict_class==test.data$HSUGBC)

# 
# summary(model)
```
```{r smote}
dat44<-dat4[,1:18] # do NOT include DIABBC and sores other than dia_score
dat44$HCHOLBC<-droplevels(dat44$HCHOLBC)
dat44<-na.omit(dat44)
table(dat44$HCHOLBC)


dat44.balanced <-smote(HCHOLBC~., dat44, perc.over=600, perc.under=2)

table(dat44.balanced$HCHOLBC)


dat44.balanced<-dat44.balanced %>% mutate(
  cho_score= ifelse(HCHOLBC==5, 0, 1),
)

dat44.balanced$cho_score<-as.factor(dat44.balanced$cho_score)


# 
# 
# dat33.balanced<-dat33.balanced[,-4]
# dat33.balanced
# 
# model<-lm(dia_score~., data=dat33.balanced)


set.seed(2021)
training.samples <- dat44.balanced$cho_score %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat44.balanced[training.samples, ]
test.data <- dat44.balanced[-training.samples, ]

dat44.balanced

#model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
model<-nnet::multinom(cho_score~.-HCHOLBC, data=train.data)
predict_class<-predict(model,newdata=test.data)



length(predict_class)
length(test.data$HCHOLBC)

summary(model)
data.frame(predict_class)

table(predict_class, test.data$cho_score)

mean(predict_class==test.data$cho_score)





training.samples <- dat44$HCHOLBC %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- dat44[training.samples, ]
test.data <- dat44[-training.samples, ]

model<-nnet::multinom(HCHOLBC~., data=train.data)


predict_class<-predict(model, newdata=test.data)
table(predict_class, test.data$HCHOLBC)



mean(predict_class==test.data$HCHOLBC)

# 
# summary(model)


```


```{r smote-k-fold cross-valdation for linear regression }
set.seed(2021)
# Predict cho_score, drop other y and y-related variables
dat4<-subset(dat4, select=-c(DIABBC,HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cvd_score, sug_score, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat44.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

#Predict sug_score
set.seed(2021)
# Predict sug_score, drop other y and y-related variables
dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC,CVDMEDST, cho_score,dia_score,cvd_score, hyp_score, final_score)) 
# str(dat5)
# dat55<-dat5[dat5$sug_score>0,]
# str(dat55)
# remove NA values
dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

#table(dat55.balanced$HYPBC)

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

```

```{r k-fold cross-validation for linear regression }
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat4<-subset(dat4, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cvd_score, sug_score,CVDMEDST, hyp_score, final_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat4$cho_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat4[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(cho_score~., data=fold_train) # full model
  M0<-lm(cho_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

# dat5 sug_score
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
#dat5<-subset(dat5, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score,CVDMEDST, hyp_score, final_score,cvd_score)) 
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
#dat5<-na.omit(dat5)
# set up k value for k-fold cross validation 
k_fold=10
# create k folds
folds<-createFolds(y=dat5$sug_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c() 

# K-fold cross-validation:
for (i in 1:k_fold){
  fold_test<-dat5[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat5[-folds[[i]],] # remaining is training set
  # linear regression using AIC
  M1<-lm(sug_score~., data=fold_train) # full model
  M0<-lm(sug_score~1, data=fold_train) # null model
  lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
  fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
  test_error_RMSE[i] = RMSE(fold_predict, fold_test$sug_score) # calculate and record RMSE
}

summary(lm_model)
#model<-lm(dia_score~., data=dat5)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
```


```{r k-fold cv for neural network}
# we use dat3 as a starting point for neural network analysis
dat10=dat3
# normalize all numeric variables
mystd <-function (x){
  x<- (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}
# remove NA
dat10<-na.omit(dat10)
# normalize numeric values in dat5 such that they have mean value = 0 and std = 1
dat10_std<-dat10%>% mutate_if(is.numeric, list(mystd))
# create dataset with one hot encoding
dat10_one_hot <- one_hot(as.data.table(dat10_std))

# set up k value for k-fold cross validation 
k_fold=10
# set number of hidden layers
hid_layer=3
# create k folds
folds<-createFolds(y=dat10_one_hot$cho_score, k=k_fold)

test_error_RMSE<-c() 

for (i in 1:k_fold){
  fold_test<-dat10_one_hot[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat10_one_hot[-folds[[i]],] # remaining is training set
  network<-neuralnet(cho_score~.-final_score-cvd_score-sug_score-hyp_score, data=fold_train, hidden =hid_layer)
  fold_predict<-predict(network, fold_test)
  test_error_RMSE[i]<-RMSE(fold_predict, fold_test$cho_score) # calculate and record RMSE
}

avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

plot(network)
```

```{r k-fold cv for neural network}
# we use dat3 as a starting point for neural network analysis
dat11=dat3
# normalize all numeric variables
mystd <-function (x){
  x<- (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}
# remove NA
dat11<-na.omit(dat11)
# normalize numeric values in dat5 such that they have mean value = 0 and std = 1
dat11_std<-dat11%>% mutate_if(is.numeric, list(mystd))
# create dataset with one hot encoding
dat11_one_hot <- one_hot(as.data.table(dat11_std))

# set up k value for k-fold cross validation 
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$sug_score, k=k_fold)

test_error_RMSE<-c() 

for (i in 1:k_fold){
  fold_test<-dat11_one_hot[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat11_one_hot[-folds[[i]],] # remaining is training set
  network<-neuralnet(sug_score~.-final_score-cho_score-cvd_score-hyp_score, data=fold_train, hidden =hid_layer)
  fold_predict<-predict(network, fold_test)
  test_error_RMSE[i]<-RMSE(fold_predict, fold_test$sug_score) # calculate and record RMSE
}

avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

plot(network)
```

```{r random forest}
dat6 = dat3
dat6<-na.omit(dat6)

print(table(dat6$cvd_score))

# set up k value for k-fold cross validation 
k_fold=10
# set number of hidden layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)

test_error_RMSE<-c() 

for (i in 1:k_fold){
  fold_test<-dat6[folds[[i]],] # select folds[[i]] as test test
  fold_train<-dat6[-folds[[i]],] # remaining is training set
  rf<-randomForest(cvd_score ~ .-final_score-cho_score-sug_score-hyp_score, data=fold_train, ntree=100)
  fold_predict<-predict(rf, fold_test)
  test_error_RMSE[i]<-RMSE(fold_predict, fold_test$cvd_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE

# fit using a single tree
rp<-rpart(dia_score~.-final_score-cho_score-sug_score-hyp_score, 
      data = dat6,
      control=rpart.control(cp=0.0001, minsplit=5)
      )

fancyRpartPlot(rp,palettes=c("Oranges"), main="Dia_score Prediction")

```


